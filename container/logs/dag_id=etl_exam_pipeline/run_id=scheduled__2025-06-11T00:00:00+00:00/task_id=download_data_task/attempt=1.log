[2025-06-12T14:21:23.708+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-12T14:21:23.785+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [queued]>
[2025-06-12T14:21:23.793+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [queued]>
[2025-06-12T14:21:23.794+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-06-12T14:21:23.808+0000] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): download_data_task> on 2025-06-11 00:00:00+00:00
[2025-06-12T14:21:23.836+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=151) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-06-12T14:21:23.839+0000] {standard_task_runner.py:63} INFO - Started process 153 to run task
[2025-06-12T14:21:23.841+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_exam_pipeline', 'download_data_task', 'scheduled__2025-06-11T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/pipeline_dag.py', '--cfg-path', '/tmp/tmpd27mxw9p']
[2025-06-12T14:21:23.846+0000] {standard_task_runner.py:91} INFO - Job 7: Subtask download_data_task
[2025-06-12T14:21:23.905+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [running]> on host 503649b6cad7
[2025-06-12T14:21:23.965+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_exam_pipeline' AIRFLOW_CTX_TASK_ID='download_data_task' AIRFLOW_CTX_EXECUTION_DATE='2025-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-11T00:00:00+00:00'
[2025-06-12T14:21:23.967+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-12T14:21:23.967+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-06-12T14:21:23.968+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'cd /opt/*** && python etl/extractor.py']
[2025-06-12T14:21:23.974+0000] {subprocess.py:86} INFO - Output:
[2025-06-12T14:21:24.255+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-06-12T14:21:24.259+0000] {subprocess.py:93} INFO -   File "/opt/***/etl/extractor.py", line 7, in <module>
[2025-06-12T14:21:24.262+0000] {subprocess.py:93} INFO -     logger = setup_logger('../logs/etl_pipeline.log')
[2025-06-12T14:21:24.263+0000] {subprocess.py:93} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-06-12T14:21:24.263+0000] {subprocess.py:93} INFO -   File "/opt/***/etl/logger.py", line 8, in setup_logger
[2025-06-12T14:21:24.263+0000] {subprocess.py:93} INFO -     os.makedirs(os.path.dirname(log_file), exist_ok=True)
[2025-06-12T14:21:24.263+0000] {subprocess.py:93} INFO -   File "<frozen os>", line 225, in makedirs
[2025-06-12T14:21:24.263+0000] {subprocess.py:93} INFO - PermissionError: [Errno 13] Permission denied: '../logs'
[2025-06-12T14:21:24.266+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-06-12T14:21:24.266+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-12T14:21:24.274+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-06-12T14:21:24.277+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_exam_pipeline, task_id=download_data_task, execution_date=20250611T000000, start_date=20250612T142123, end_date=20250612T142124
[2025-06-12T14:21:24.284+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 7 for task download_data_task (Bash command failed. The command returned a non-zero exit code 1.; 153)
[2025-06-12T14:21:24.314+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-06-12T14:21:24.327+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-12T14:21:24.328+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-12T14:37:03.562+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-12T14:37:03.570+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [queued]>
[2025-06-12T14:37:03.573+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [queued]>
[2025-06-12T14:37:03.573+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-06-12T14:37:03.578+0000] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): download_data_task> on 2025-06-11 00:00:00+00:00
[2025-06-12T14:37:03.585+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=95) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-06-12T14:37:03.586+0000] {standard_task_runner.py:63} INFO - Started process 97 to run task
[2025-06-12T14:37:03.588+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_exam_pipeline', 'download_data_task', 'scheduled__2025-06-11T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/pipeline_dag.py', '--cfg-path', '/tmp/tmpcpgxuscm']
[2025-06-12T14:37:03.592+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask download_data_task
[2025-06-12T14:37:03.619+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [running]> on host 7c490d104b7c
[2025-06-12T14:37:03.657+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_exam_pipeline' AIRFLOW_CTX_TASK_ID='download_data_task' AIRFLOW_CTX_EXECUTION_DATE='2025-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-11T00:00:00+00:00'
[2025-06-12T14:37:03.658+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-12T14:37:03.658+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-06-12T14:37:03.659+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'cd /opt/***/ && python etl/extractor.py']
[2025-06-12T14:37:03.663+0000] {subprocess.py:86} INFO - Output:
[2025-06-12T14:37:03.922+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-06-12T14:37:03.922+0000] {subprocess.py:93} INFO -   File "/opt/***/etl/extractor.py", line 7, in <module>
[2025-06-12T14:37:03.923+0000] {subprocess.py:93} INFO -     logger = setup_logger('../logs/etl_pipeline.log')
[2025-06-12T14:37:03.923+0000] {subprocess.py:93} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-06-12T14:37:03.923+0000] {subprocess.py:93} INFO -   File "/opt/***/etl/logger.py", line 8, in setup_logger
[2025-06-12T14:37:03.923+0000] {subprocess.py:93} INFO -     os.makedirs(os.path.dirname(log_file), exist_ok=True)
[2025-06-12T14:37:03.923+0000] {subprocess.py:93} INFO -   File "<frozen os>", line 225, in makedirs
[2025-06-12T14:37:03.923+0000] {subprocess.py:93} INFO - PermissionError: [Errno 13] Permission denied: '../logs'
[2025-06-12T14:37:03.932+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-06-12T14:37:03.932+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-12T14:37:03.938+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-06-12T14:37:03.941+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_exam_pipeline, task_id=download_data_task, execution_date=20250611T000000, start_date=20250612T143703, end_date=20250612T143703
[2025-06-12T14:37:03.945+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 3 for task download_data_task (Bash command failed. The command returned a non-zero exit code 1.; 97)
[2025-06-12T14:37:03.984+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-06-12T14:37:03.995+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-12T14:37:03.996+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-12T14:47:13.332+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-12T14:47:13.342+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [queued]>
[2025-06-12T14:47:13.345+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [queued]>
[2025-06-12T14:47:13.346+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 2
[2025-06-12T14:47:13.351+0000] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): download_data_task> on 2025-06-11 00:00:00+00:00
[2025-06-12T14:47:13.368+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=75) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-06-12T14:47:13.370+0000] {standard_task_runner.py:63} INFO - Started process 77 to run task
[2025-06-12T14:47:13.370+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_exam_pipeline', 'download_data_task', 'scheduled__2025-06-11T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/pipeline_dag.py', '--cfg-path', '/tmp/tmpeq4hi1tu']
[2025-06-12T14:47:13.372+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask download_data_task
[2025-06-12T14:47:13.405+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_exam_pipeline.download_data_task scheduled__2025-06-11T00:00:00+00:00 [running]> on host 856a94aade24
[2025-06-12T14:47:13.444+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_exam_pipeline' AIRFLOW_CTX_TASK_ID='download_data_task' AIRFLOW_CTX_EXECUTION_DATE='2025-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-11T00:00:00+00:00'
[2025-06-12T14:47:13.445+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-12T14:47:13.446+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-06-12T14:47:13.446+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'cd /opt/***/etl/ && python extractor.py']
[2025-06-12T14:47:13.453+0000] {subprocess.py:86} INFO - Output:
[2025-06-12T14:47:13.746+0000] {subprocess.py:93} INFO - 2025-06-12 14:47:13,746 - etl_pipeline - INFO - Начало загрузки данных...
[2025-06-12T14:47:15.548+0000] {subprocess.py:93} INFO - 2025-06-12 14:47:15,547 - etl_pipeline - INFO - Данные успешно загружены в ../results/raw/wdbc.data
[2025-06-12T14:47:15.567+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-06-12T14:47:15.569+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-12T14:47:15.595+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=etl_exam_pipeline, task_id=download_data_task, execution_date=20250611T000000, start_date=20250612T144713, end_date=20250612T144715
[2025-06-12T14:47:15.649+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-12T14:47:15.670+0000] {taskinstance.py:3482} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-12T14:47:15.671+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
